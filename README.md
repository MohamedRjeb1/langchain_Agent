# Local YouTube ‚Üí RAG (fully local) üõ†Ô∏è

Pipeline local complet pour interroger le contenu de vid√©os YouTube via RAG, sans services cloud:
- yt-dlp + ffmpeg pour l‚Äôaudio
- Whisper pour la transcription (local)
- Chunking s√©mantique dynamique avec seuil adaptatif
- Embeddings locaux via Ollama (nomic-embed-text)
- Index FAISS optimis√© (cosine via L2)
- LLM local (Mistral via Ollama)
- Corrective RAG (auto-grade + query rewrites)
- M√©moire pour am√©liorer la r√©cup√©ration (sans injecter la m√©moire dans le prompt)
- UI Streamlit ‚Äúchat‚Äù avec panneau de logs enrichi √† droite


## Aper√ßu rapide

- Interface chat Streamlit: √† gauche le chat, √† droite un large panneau de logs d√©taillant l‚Äôingestion, le chunking, l‚Äôindexation et la r√©cup√©ration.
- Ingestion YouTube ‚Üí audio mp3 ‚Üí transcription Whisper ‚Üí chunking s√©mantique ‚Üí embeddings Ollama ‚Üí index FAISS + clustering (KMeans + silhouette).
- R√©cup√©ration: FAISS + l√©ger rerank + diagnostics (faiss_similarity, rank, cluster_id) + Corrective RAG (grading + r√©√©critures) + r√©sum√© de preuves si trop long.
- M√©moire: les requ√™tes et les Q/R sont vectoris√©es et utilis√©es pour booster la r√©cup√©ration (centro√Øde + sous-requ√™tes). La m√©moire n‚Äôest pas inject√©e textuellement dans le prompt final.


## Architecture et services

Le code se trouve principalement sous `app/services/`.

- YouTubeService (`youtube_service.py`)
	- T√©l√©charge l‚Äôaudio via yt-dlp, applique une extraction FFmpeg en mp3, prend en charge un `progress_hook` pour les logs de progression.
	- Sorties: chemin du fichier audio, titre/metadata, statut.

- TranscriptionService (`transcription_service.py`)
	- Transcrit l‚Äôaudio en texte via Whisper.
	- Utilise `ModelLoaderService` pour charger et mettre en cache le mod√®le (typiquement `small` par d√©faut) et journalise les √©tapes.
	- Sauvegarde le transcript dans `data/transcripts/{task_id}_transcript.txt`.

- SemanticChunkingService (`semantic_chunking_service.py`)
	- Chunking s√©mantique dynamique avec:
		- Seuil de base: 0.65
		- Seuil adaptatif bas√© sur la variance locale des similarit√©s inter-segments
		- Taille min: 100 caract√®res; taille max: 2000 caract√®res
	- Similarit√© mesur√©e via embeddings (LocalEmbeddingService) + cosinus.
	- Journalisation fine: progression, similarit√© entre segments cons√©cutifs (born√©e pour √©viter le spam sur tr√®s longs textes), finalisation de chaque chunk avec stats (segments, caract√®res, seuil).

- LocalEmbeddingService (`embedding_service.py`)
	- Embeddings locaux via Ollama (`nomic-embed-text`, dimension 768), avec un petit cache.
	- `embed_query` et `embed_documents` expos√©s; messages d‚Äôerreur non bloquants.

- FAISSIndex (`faiss_index.py`)
	- Index FAISS `IndexFlatIP` avec normalisation L2 (cosine ‚âà inner product apr√®s L2).
	- Scores normalis√©s dans [0,1]. Persistence via `faiss.write_index` + JSON pour la map doc.
	- Verrous de fichier pour √©viter les corruptions concurrentes; reconstruction de vecteurs pour un l√©ger reranking.

- AdvancedIndexingService (`indexing_service.py`)
	- Construction d‚Äôun index FAISS √† partir des embeddings et des objets documents.
	- Clustering s√©mantique via KMeans (n_init=10) + silhouette; clusters et m√©tadonn√©es persist√©s.
	- `search_similar(...)`:
		- Cherche dans FAISS, conserve `faiss_similarity`, calcule un sim coh√©rent via vecteurs reconstruits.
		- Retourne: `content`, `metadata`, `similarity_score`, `faiss_similarity`, `cluster_id`, `rank`.
	- `get_index_statistics(...)`, `load_index(...)`, `search_by_cluster(...)` (recherche textuelle simple par cluster).

- LLMService (`llm_service.py`)
	- G√©n√©ration via `langchain_ollama.OllamaLLM` (Mistral) avec repli HTTP si besoin.
	- Param√®tres: `max_tokens`, `temperature`.

- CombinedRAGService (`rag_service.py`)
	- Ingestion de bout en bout: download ‚Üí transcribe ‚Üí chunk ‚Üí embed ‚Üí index.
	- R√©ponse √† une requ√™te: construit un retriever FAISS, ex√©cute la r√©cup√©ration + Corrective RAG + m√©moire.
	- Corrective RAG: grading LLM des ‚Äústrips‚Äù r√©cup√©r√©s (robuste aux sorties JSON encapsul√©es), r√©√©critures si le meilleur grade < seuil, puis re‚Äëgrade.
	- M√©moire: stockage de la requ√™te et de la Q/R, r√©utilisation pour booster la r√©cup√©ration (fusion centro√Øde et sous‚Äërequ√™tes) uniquement; pas d‚Äôinjection textuelle en prompt.
	- R√©sum√© optionnel des preuves si le texte d√©passe un budget (compression via LLM avant r√©ponse).

- RouterService (`router_service.py`)
	- Classifieur (LLM ou heuristiques) ‚ÄúRAG‚Äù vs ‚ÄúNORMAL‚Äù. L‚ÄôUI actuelle force ‚ÄúRAG‚Äù pour chaque r√©ponse; le router reste disponible si besoin.


## Strat√©gies de chunking

Le chunking est s√©mantique et dynamique:
- On segmente en phrases/paragraphes, puis on calcule la similarit√© cosinus entre segments cons√©cutifs via les embeddings locaux.
- Seuil adaptatif: on abaisse/hausse l√©g√®rement le seuil de base (0.65) selon la variance locale des similarit√©s (fen√™tre r√©cente), born√© √† [0.3, 0.9].
- Fin d‚Äôun chunk si: similarit√© < seuil adaptatif ou si la taille max (2000 chars) serait d√©pass√©e; on respecte aussi une taille minimale (100 chars) pour valider un chunk.
- Chaque chunk porte des m√©tadonn√©es: `task_id`, `chunk_id`, `chunk_index`, `segment_count`, `character_count`, `similarity_threshold_used`, etc.


## Strat√©gies de retrieval

1) FAISS (cosine normalis√©e): on r√©cup√®re k candidats avec similarit√© ‚àà [0,1] (FAISS).
2) Rerank l√©ger: on reconstruit les vecteurs des top-k et on recalcule un cosinus exact vs le vecteur de requ√™te normalis√© (toujours born√© dans [0,1]).
3) Diagnostics attach√©s pour la transparence:
	 - `similarity_score` (post-rerank), `faiss_similarity` (score FAISS initial), `rank` (rang final), `cluster_id` (si clustering existant).
4) M√©moire pour le boost de pr√©cision (jamais inject√©e au prompt):
	 - Fusion centro√Øde (requ√™te + centroid des souvenirs proches) ‚Üí recherche directe par vecteur.
	 - Sous-requ√™tes d√©riv√©es des souvenirs Q/A les plus proches (multi-query retrieval).
5) Corrective RAG (CRAG):
	 - Grading LLM des strips (sortie JSON robuste aux ```json ‚Ä¶ ``` et aux textes parasites).
	 - Si le meilleur grade < seuil (ex: 0.6), on g√©n√®re N r√©√©critures, on relance des recherches, on fusionne + re‚Äëgrade.
	 - On retient ensuite les top-k par (grade, similarity).


## UI Streamlit (chat + logs)

- Chat √† gauche (toujours RAG), logs √† droite (colonne plus large pour mieux lire les diagnostics).
- Ingestion: saisissez l‚ÄôURL YouTube et un ‚ÄúVideo title‚Äù (servira de `task_id`). Suivez la progression dans les logs (download, transcription, chunking, embeddings, index, clustering).
- Chat: posez vos questions sur la vid√©o. Les logs montrent les top r√©sultats avec rang, similarit√©s (FAISS vs rerank), cluster et id de chunk.
- Par d√©faut, la ‚Äúprovenance‚Äù n‚Äôest pas affich√©e dans le chat (elle reste disponible dans les logs).


## Structure du projet

```
app/
	core/config.py                # Config pydantic (chemins, mod√®les par d√©faut, etc.)
	services/
		youtube_service.py          # yt-dlp + ffmpeg ‚Üí mp3, avec hooks de progression
		transcription_service.py    # Whisper, persistance transcript
		semantic_chunking_service.py# Chunking s√©mantique dynamique + logs
		embedding_service.py        # Ollama embeddings (nomic-embed-text)
		faiss_index.py              # Wrapper FAISS (L2, save/load, reconstruct)
		indexing_service.py         # Build/Load FAISS + clustering + search_similar
		llm_service.py              # Ollama LLM (Mistral) avec fallback HTTP
		rag_service.py              # Orchestration ingestion + RAG + m√©moire + CRAG
		router_service.py           # Router (non utilis√© par l‚ÄôUI actuelle)

streamlit_app.py                # UI chat + logs (colonne droite √©largie)
data/
	audio/                        # mp3 t√©l√©charg√©s
	transcripts/                  # {task_id}_transcript.txt
	VectorStore/                  # {task_id}.faiss, {task_id}_docs.json, metadata
```


## Pr√©requis

- Python 3.10+
- [Ollama](https://ollama.com/) install√© et en cours d‚Äôex√©cution
	- Mod√®les requis: `mistral` (ou un mod√®le Mistral local √©quivalent) et `nomic-embed-text`
- ffmpeg install√© (pour yt-dlp ‚Üí mp3)


## Installation (Windows PowerShell)

```powershell
# 1) Cloner ce d√©p√¥t (adapt√© si vous n‚Äô√™tes pas d√©j√† dans le dossier)
# git clone <repository-url>
# cd la

# 2) Cr√©er et activer l‚Äôenvironnement
python -m venv venv
venv\Scripts\Activate

# 3) Installer les d√©pendances Python
pip install -r requirements.txt

# 4) S‚Äôassurer qu‚ÄôOllama tourne et que les mod√®les sont pr√©sents
# (dans un autre terminal)
# ollama serve
# ollama pull mistral
# ollama pull nomic-embed-text

# 5) ffmpeg doit √™tre accessible dans le PATH
# (optionnel) winget install Gyan.FFmpeg

# 6) Lancer l‚ÄôUI
streamlit run streamlit_app.py
```

Variables de configuration optionnelles via `.env` (charg√© par pydantic):

```
DEFAULT_LLM_MODEL=mistral
DEFAULT_EMBEDDING_MODEL=nomic-embed-text
WHISPER_MODEL=small
WHISPER_LANGUAGE=en
VECTORSTORE_DIR=data/VectorStore
CHUNK_SIZE=512
CHUNK_OVERLAP=64
```


## Guide de test simple

1) Lancez l‚ÄôUI Streamlit: la page s‚Äôouvre dans le navigateur.
2) Saisissez une URL YouTube et un ‚ÄúVideo title‚Äù (ex: `video_1`), cliquez ‚ÄúStart ingestion‚Äù.
	 - Surveillez la colonne ‚ÄúLogs‚Äù: progression du download, transcription Whisper, chunking s√©mantique (sim, seuils, finalisation), embeddings, index FAISS, clustering.
3) Une fois l‚Äôingestion termin√©e, posez des questions dans le chat.
	 - Les logs montrent les diagnostics de r√©cup√©ration: rang, similitudes FAISS vs rerank, cluster, chunk_id.
	 - Si le grading JSON √©choue, un log explique que des notes 0.00 sont utilis√©es par d√©faut; sinon vous verrez des grades non nuls.
4) Relancez des questions: la m√©moire (queries/QA) va progressivement am√©liorer la r√©cup√©ration via centro√Øde et sous‚Äërequ√™tes (sans polluer le prompt).


## D√©pannage

- ‚ÄúOllamaLLM not available‚Äù ou erreurs de g√©n√©ration: v√©rifiez que `langchain_ollama` est install√© et qu‚ÄôOllama tourne (mod√®les pull).
- ‚Äúfaiss non install√©‚Äù: installez `faiss-cpu` correspondant √† votre plateforme.
- yt-dlp/ffmpeg: v√©rifiez que `ffmpeg` est install√© et accessible dans le PATH.
- Whisper mod√®le: le premier run peut t√©l√©charger le mod√®le; si n√©cessaire utilisez la fonction de pr√©chargement ou d√©clenchez une transcription pour le mettre en cache.
- Grades toujours √† 0.00: c‚Äô√©tait souvent d√ª √† des sorties JSON encapsul√©es par le LLM; le parsing est maintenant robuste aux ```json ...``` et au texte additionnel. V√©rifiez les logs c√¥t√© ‚Äú[llm] Grading ‚Ä¶‚Äù pour voir l‚Äô√©tat.


## Licence

Usage et redistribution sous la licence du projet.

